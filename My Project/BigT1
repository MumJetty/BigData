
======================================

(4/10)

------------------------------------------------------

### 작업형2 예시 문제 (타이타닉) 

# 아래는 타이타닉호 탑승자 생존과 관련한 데이터. 주어진 데이터를 이용하여 예측모형 만들고 CSV파일 생성하시오

# (가) 제공 데이터 목록 
  #(1) titanic3_y_train.csv : 탑승자의 생존 여부 데이터(학습용), csv 형식의 파일 
  #(2) titanic3_X_train.csv , titanic3_X_test.csv : 탑승자의 주요 속성 (학습용 및 평가용), csv 형식의 파일 

# (나) 데이터 형식 및 내용 
  #(1) titanic3_y_train.csv (785명 데이터) 
       # ID : 각 탑승자 고유번호 // survived : 생존 여부 (0: 사망 , 1: 생존) 
  #(2) titanic3_X_train.csv (785명 학습용 데이터) , titanic3_X_test.csv (524명 평가용 데이터)  

# 탑승자 785명에 대한 학습용 데이터(titanic3_y_train.csv , titanic3_X_train.csv)를 이용하여 , 생존 여부 예측모형을 만든 후 
# 이를 평가용 데이터(titanic3_X_test.csv)에 적용하여 얻은 524명 탑승자의 생존 여부 예측값을 CSV 파일로 생성하시오. 
# (제출한 모델의 성능은 F1-score 평가지표에 따라 채점) 

# <제출형식> 
# ID : 각 탑승자의 고유번호 // survived : 생존 여부 (0: 사망 , 1: 생존)  

# <유의사항>
# 성능이 우수한 예측모형을 구축하기 위해서 
# 적절한 데이터 전처리 , Feature Engineering , 분류 알고리즘 사용 , 초매개변수 최적화 , 모형 앙상블 등 수반


import pandas as pd

X_train = pd.read_csv(r'C:\빅데이터분석기사 실기 모의고사\data\모의고사\01회/titanic3_X_train.csv')
X_test = pd.read_csv(r'C:\빅데이터분석기사 실기 모의고사\data\모의고사\01회/titanic3_X_test.csv')
y_train = pd.read_csv(r'C:\빅데이터분석기사 실기 모의고사\data\모의고사\01회/titanic3_y_train.csv')

print(X_train.head())
print(X_test.head())
print(y_train.head())

print(X_train.info())
print(X_test.info())
print(y_train.info())

print(X_train.describe())
print(X_test.describe())
print(y_train.describe())


# 결과 제출 시 X_test의 컬럼이 필요하기 때문에 별도 저장 
ID = X_test['ID'].copy()

# 별도 저장한 컬럼 및 불필요한 컬럼 삭제 
X_train = X_train.drop(columns = ['ID','name'])
X_test = X_test.drop(columns = ['ID','name'])
y_train = y_train.drop(columns = ['ID'])


# >>> 결측치 처리 

# 결측치 확인 
X_train.isna().sum()
X_test.isna().sum() 


# 결측일 조건 
cond_na = X_train['age'].isna() 


# 피어슨 상관계수: +1이면 양의 상관관계, -1이면 음의 상관관계, 0의 경우 상관관계 갖지 않음

from scipy.stats import pearsonr 
pearsonr(y_train['survived'][~cond_na],X_train['age'][~cond_na])

# age는 탑승자의 나이를 의미하고 survived와 상관관계가 낮으므로 컬럼을 삭제 
X_train = X_train.drop('age', axis = 1) 
X_test = X_test.drop('age', axis = 1) 


# 교재풀이 때문에 확인해 봄
y_train.isna().sum()


# fare는 티겟요금을 의미하고, train에만 존재하므로 레코드를 삭제함 
# 결측일 조건 
# cond_na = X_train['fare'].isna() 

# 행 삭제 
# X_train = X_train[~cond_na]
# y_train = y_train[~cond_na]


X_train = X_train.drop('fare', axis=1)
X_test = X_test.drop('fare', axis=1) 


# cabin은 선실번호를 의미하고 train은 레코드의 78%, test는 레코드의 76%가 결측이므로 컬럼을 삭제 

# cabin 컬럼 삭제 
X_train = X_train.drop('cabin' , axis = 1) 
X_test = X_test.drop('cabin', axis =1) 


# ebarked는 탑승한 곳을 의미하고, 범주형(C,Q,S)으로, 최다빈도를 가지는 범주로 대체함 

top = X_train['embarked'].value_counts().idxmax() 


print(top)


# 대치 

X_train['embarked'] = X_train['embarked'].fillna(top) 
X_test['embarked'] = X_test['embarked'].fillna(top)  



# >> 카테고리형 컬럼 전처리 

# 문자열(object) 컬럼들의 유일값 수 확인  

print(X_train.select_dtypes('object').nunique())
print(X_test.select_dtypes('object').nunique())


# 컬럼 중 sex컬럼, 여성에 대한 일부 카테고리가 F로 되어있음 

X_train['sex'].value_counts()
X_test['sex'].value_counts()


# train, test 모두 'F'를 'female'로 통일 

X_train['sex'] = X_train['sex'].map({'male':'male','female':'female','F':'female'}) 
X_test['sex'] = X_test['sex'].map({'male':'male','female':'female','F':'female'})


# ticket 컬럼 
# 대다수가 중복되지 않으므로 (어느 한 MC로 그룹핑이 안되고 특징이 없음), 컬럼을 삭제하는 것으로 결정 

X_train = X_train.drop('ticket', axis = 1) 
X_test = X_test.drop('ticket', axis = 1) 


# >>> 수치형 컬럼 전처리 
# pclass 컬럼 
# 수치형으로 인식되지만 1,2,3등석 정보를 각 1,2,3으로 저장한 것으로 
# 카테고리의 의미를 가지는 컬럼 (즉, 문자가 문자가 아닌 숫자 형태로 되어있음) 
# dtype 변경 후 파생변수 pclass_gp에 할당하고 기존 컬럼 삭제  

X_train['pclass_gp'] = X_train['pclass'].astype('object')
X_test['pclass_gp'] = X_test['pclass'].astype('object') 


# 데이터 타입 변경한 것 변수에 담기를 완료 후 기존 것 삭제 

X_train = X_train.drop('pclass', axis = 1)
X_test = X_test.drop('pclass', axis = 1) 


# sibsp, parch 컬럼 
# sibsp는 동승한 형제 또는 배우자수, parch는 동승한 부모 또는 자녀의 수이므로 
# 두 컬럼을 합한 파생변수 fam을 생성하고 이는 동승한 가족 인원을 의미  

X_train['fam'] = X_train['sibsp'] + X_train['parch'] 
X_test['fam'] = X_test['sibsp'] + X_test['parch'] 


# 파생변수 생성 후 기존 것 삭제 

X_train = X_train.drop(['sibsp','parch'], axis = 1)
X_test = X_test.drop(['sibsp','parch'], axis = 1) 


from sklearn.model_selection import train_test_split 

# >>> 데이터 분할 

# X_train과 y_train을 학습용 (X_TRAIN, y_TRAIN)과 검증용 (X_VAL, y_VAL)로 분할 
# ( , , , ,) : 변수에 담기 

X_TRAIN, X_VAL, y_TRAIN, y_VAL = train_test_split(X_train, y_train, random_state = 1234, test_size = 0.1) 


# 분할 후 shpae 확인 

print(X_TRAIN.shape)
print(X_VAL.shape)
print(y_TRAIN.shape)
print(y_VAL.shape)


# 인코딩 - 원-핫 인코딩 수행 
# 카테고리형 컬럼(object)에 대하여 원핫인코딩 수행 
# 원핫인코딩 : 범주형 데이터를 숫자로 변환하여, 딥러닝 모델에 적용 가능하게 변환함 
# (ex) 개 1000, 고양이 0100, 원숭이 0010, 돼지 0001 

from sklearn.preprocessing import OneHotEncoder 

# 인코딩 할 카테고리형(object) 컬럼만 별도 저장   

X_TRAIN_category = X_TRAIN.select_dtypes('object').copy() 
X_VAL_category = X_VAL.select_dtypes('object').copy()
X_TEST_category = X_test.select_dtypes('object').copy() 


# 원-핫 인코딩 
# 디폴트는 sparse = True이며 매트릭스 리턴 , sparse = False이면 배열 리턴 
# fit 메서드: 모델을 학습시킬 때 사용함. // 머신러닝이 데이터에 머신러닝 모델을 맞추는 것(fit) 
# transform: fit을 통해 세운 기준(모델)으로 맞춰서 변형하는 함수  

enc = OneHotEncoder(sparse = False).fit(X_TRAIN_category)

X_TRAIN_OH = enc.transform(X_TRAIN_category)
X_VAL_OH = enc.transform(X_VAL_category) 
X_TEST_OH = enc.transform(X_TEST_category) 


# >>> 스케일링 
# 스케일링 ;; 카테고리형이 아닌 것, 숫자 관련된 것. 수치형. 
# StandardScaler(): 표준화. 평균이 0이고 분산 1인 정규분포 만드는 것. z-score  

from sklearn.preprocessing import StandardScaler 

# 스케일링할 컬럼만 별도 저장 
# select_dtypes() 메서드의 exclude 옵션은 해당 dtype을 제외한 모든 dtypes를 추출할 때 사용 
# 테스트는 스플릿이 안되어 있으므로 정의변수는 대문자, 할당 시에는 소문자 사용 

X_TRAIN_conti = X_TRAIN.select_dtypes(exclude = 'object').copy() 
X_VAL_conti = X_VAL.select_dtypes(exclude = 'object').copy() 
X_TEST_conti = X_test.select_dtypes(exclude = 'object').copy() 


# TRAIN 데이터 기준으로 스케일링 함 

scale = StandardScaler().fit(X_TRAIN_conti) 


# Z-점수 표준화 
# ; transform에서 평균, 분산, 표준편차 등 가지고, 거기에 때리면서 구하면서 표준화 
# transform: fit을 통해 세운 기준(모델)으로 맞춰서 변형하는 함수  

X_TRAIN_STD = scale.transform(X_TRAIN_conti)
X_VAL_STD = scale.transform(X_VAL_conti)
X_TEST_STD = scale.transform(X_TEST_conti) 


# >>> 입력 데이터셋 준비  

import numpy as np 

# 인코딩과 스케일링 된 넘파이 배열 연결  

X_TRAIN = np.concatenate([X_TRAIN_OH, X_TRAIN_STD],axis = 1) 
X_VAL = np.concatenate([X_VAL_OH, X_VAL_STD],axis = 1) 


# y를 1차원 넘파이 배열로 평탄화 
# ravel: 1차원 배열로 평평하게 펴주는 함수 

# (참고) 
# 딕셔너리는 key(), values() 
# 그 중 키만 뽑을 때 : key()
# 값만 뽑을 때 : values() 
# 키, 값 쌍으로 뽑고 싶을 때 : items() 


# 1차원 넘파이 배열로 평탄화 
y_TRAIN = y_TRAIN.values.ravel() 
y_VAL = y_VAL.values.ravel() 


# >>> 모델 학습 

from sklearn.ensemble import RandomForestClassifier 


 랜덤 포레스트 

rf = RandomForestClassifier(n_estimators = 500, max_depth = 3, min_samples_leaf = 10, max_features = 'sqrt', random_state = 2022) 

model_rf = rf.fit(X_TRAIN, y_TRAIN) 


# 성능평가(기준:f1-score) 통한 모델 선정 // 단, 여기서는 랜덤포레스트만 사용함 

from sklearn.metrics import f1_score 


# 검증용 데이터셋을 통한 예측 

pred_rf = model_rf.predict(X_VAL) 


# f1-score 계산 

f1_rf = f1_score(y_VAL, pred_rf)

print(f1_rf)


# 결과 제출하기 

X_TEST = np.concatenate([X_TEST_OH, X_TEST_STD],axis = 1)

y_pred = model_rf.predict(X_TEST)


# 문제에서 요구하는 형태로 변환 
# <제출형식> ID , survived 

obj = {'ID':ID, 'survived':y_pred} 

result = pd.DataFrame(obj) 


# 12345.csv로 저장하기
# index = False 시 인덱스를 포함하지 않고 저장 

result.to_csv("12345.csv",index = False) 







------------------------------------------------------











------------------------------------------------------

# 작업형 1 예시 문제 
# mtcars 데이터셋(data/mtcars)의 qsec컬럼을 최소최대 척도 (Min-Max Scale)로 변환한 후 0.5보다 큰 값을 가지는 레코드 수를 구하시오. 
# dataset 위치 : data/mtcars.csv  

import pandas as pd 
import numpy as np 

df1 = pd.read_csv(r'C:/빅데이터분석기사 실기 모의고사/불러오기 파일들/mtcars.csv')
print(df1)

print(df1['qsec'].head(3))

from sklearn.preprocessing import minmax_scale 

df1['qsec'] = minmax_scale(df1['qsec'])

print(df1['qsec'].head(3)) 

print(df1['qsec'] > 0.5)
# True값만 더해서 결과값 반환 

# 최종제출 전에 중간에 한 프린트문은 주석 처리 
# 주석처리 단축키 : 컨트롤 슬러시  


# 방법2

# 자체적으로 함수 만들기 

def minmax(df1) : 
    df1 = (df1 - min(df1)) / (max(df1) - min(df1))
    return df1  

df1['qsec'] = minmax(df1['qsec'])

print(df1['qsec'].head(3)) 

print(df1[df1['qsec'] > 0.5]) 

print(len(df1[df1['qsec'] > 0.5]))


------------------------------------------------------
-.
pd.read_csv 하고 괄호 안에
앞에 r 붙여주기

exam1=pd.read_csv(r'C:\빅데이터분석기사 실기 모의고사\data\모의고사\01회\iris.csv') 

-. 비쥬얼스튜디오코드에서 작업판 만들 때 
뒤에 확장자 .ipynb로 저장 
------------------------------------------------------

======================================
