[ Big Mum P ]

================================================

[ Big T 1 ]

### 작업형2 예시 문제 (타이타닉) 

# 아래는 타이타닉호 탑승자 생존과 관련한 데이터. 주어진 데이터를 이용하여 예측모형 만들고 CSV파일 생성하시오

# (가) 제공 데이터 목록 
  #(1) titanic3_y_train.csv : 탑승자의 생존 여부 데이터(학습용), csv 형식의 파일 
  #(2) titanic3_X_train.csv , titanic3_X_test.csv : 탑승자의 주요 속성 (학습용 및 평가용), csv 형식의 파일 

# (나) 데이터 형식 및 내용 
  #(1) titanic3_y_train.csv (785명 데이터) 
       # ID : 각 탑승자 고유번호 // survived : 생존 여부 (0: 사망 , 1: 생존) 
  #(2) titanic3_X_train.csv (785명 학습용 데이터) , titanic3_X_test.csv (524명 평가용 데이터)  

# 탑승자 785명에 대한 학습용 데이터(titanic3_y_train.csv , titanic3_X_train.csv)를 이용하여 , 생존 여부 예측모형을 만든 후 
# 이를 평가용 데이터(titanic3_X_test.csv)에 적용하여 얻은 524명 탑승자의 생존 여부 예측값을 CSV 파일로 생성하시오. 
# (제출한 모델의 성능은 F1-score 평가지표에 따라 채점) 

# <제출형식> 
# ID : 각 탑승자의 고유번호 // survived : 생존 여부 (0: 사망 , 1: 생존)  

# <유의사항>
# 성능이 우수한 예측모형을 구축하기 위해서 
# 적절한 데이터 전처리 , Feature Engineering , 분류 알고리즘 사용 , 초매개변수 최적화 , 모형 앙상블 등 수반


import pandas as pd 

X_train = pd.read_csv(r'C:\빅데이터분석기사 실기 모의고사\data\모의고사\01회\titanic3_X_train.csv') 
X_test = pd.read_csv(r'C:\빅데이터분석기사 실기 모의고사\data\모의고사\01회\titanic3_X_test.csv')
y_train = pd.read_csv(r'C:\빅데이터분석기사 실기 모의고사\data\모의고사\01회\titanic3_y_train.csv') 



print(X_train.head())
print(X_test.head())
print(y_train.head())

print(X_train.info())
print(X_test.info())
print(y_train.info())

print(X_train.describe())
print(X_test.describe())
print(y_train.describe())



# 결과 제출 시 X_test의 컬럼이 필요하기 때문에 별도 저장 
ID = X_test['ID'].copy()

# 별도 저장한 컬럼 및 불필요한 컬럼 삭제 
X_train = X_train.drop(columns = ['ID','name'])
X_test = X_test.drop(columns = ['ID','name'])
y_train = y_train.drop(columns = ['ID'])



# >>> 결측치 처리 

# 결측치 확인 
X_train.isna().sum()
X_test.isna().sum() 


# 결측일 조건 
cond_na = X_train['age'].isna() 



# 피어슨 상관계수: +1이면 양의 상관관계, -1이면 음의 상관관계, 0의 경우 상관관계 갖지 않음

from scipy.stats import pearsonr 
pearsonr(y_train['survived'][~cond_na],X_train['age'][~cond_na])

# age는 탑승자의 나이를 의미하고 survived와 상관관계가 낮으므로 컬럼을 삭제 
X_train = X_train.drop('age', axis = 1) 
X_test = X_test.drop('age', axis = 1) 



# fare는 티겟요금을 의미하고, train에만 존재하므로 레코드를 삭제함 
# 결측일 조건 
# cond_na = X_train['fare'].isna() 


# 행 삭제 
X_train = X_train.drop('fare', axis=1)
X_test = X_test.drop('fare', axis=1) 


# cabin은 선실번호를 의미하고 train은 레코드의 78%, test는 레코드의 76%가 결측이므로 컬럼을 삭제 

# cabin 컬럼 삭제 
X_train = X_train.drop('cabin' , axis = 1) 
X_test = X_test.drop('cabin', axis =1) 


# ebarked는 탑승한 곳을 의미하고, 범주형(C,Q,S)으로, 최다빈도를 가지는 범주로 대체함 

top = X_train['embarked'].value_counts().idxmax() 


print(top)


# 대치 
X_train['embarked'] = X_train['embarked'].fillna(top) 
X_test['embarked'] = X_test['embarked'].fillna(top)  


# >> 카테고리형 컬럼 전처리 

# 문자열(object) 컬럼들의 유일값 수 확인  

print(X_train.select_dtypes('object').nunique())
print(X_test.select_dtypes('object').nunique()) 


# 컬럼 중 sex컬럼, 여성에 대한 일부 카테고리가 F로 되어있음 

X_train['sex'].value_counts()
X_test['sex'].value_counts()


# train, test 모두 'F'를 'female'로 통일 

X_train['sex'] = X_train['sex'].map({'male':'male','female':'female','F':'female'}) 
X_test['sex'] = X_test['sex'].map({'male':'male','female':'female','F':'female'})


# ticket 컬럼 
# 대다수가 중복되지 않으므로 (어느 한 MC로 그룹핑이 안되고 특징이 없음), 컬럼을 삭제하는 것으로 결정 

X_train = X_train.drop('ticket', axis = 1) 
X_test = X_test.drop('ticket', axis = 1) 



# >>> 수치형 컬럼 전처리 
# pclass 컬럼 
# 수치형으로 인식되지만 1,2,3등석 정보를 각 1,2,3으로 저장한 것으로 
# 카테고리의 의미를 가지는 컬럼 (즉, 문자가 문자가 아닌 숫자 형태로 되어있음) 
# dtype 변경 후 파생변수 pclass_gp에 할당하고 기존 컬럼 삭제  

X_train['pclass_gp'] = X_train['pclass'].astype('object')
X_test['pclass_gp'] = X_test['pclass'].astype('object') 


# 데이터 타입 변경한 것 변수에 담기를 완료 후 기존 것 삭제 
X_train = X_train.drop('pclass', axis = 1)
X_test = X_test.drop('pclass', axis = 1) 



# sibsp, parch 컬럼 
# sibsp는 동승한 형제 또는 배우자수, parch는 동승한 부모 또는 자녀의 수이므로 
# 두 컬럼을 합한 파생변수 fam을 생성하고 이는 동승한 가족 인원을 의미  

X_train['fam'] = X_train['sibsp'] + X_train['parch'] 
X_test['fam'] = X_test['sibsp'] + X_test['parch'] 


# 파생변수 생성 후 기존 것 삭제 
X_train = X_train.drop(['sibsp','parch'], axis = 1)
X_test = X_test.drop(['sibsp','parch'], axis = 1) 



from sklearn.model_selection import train_test_split 

# >>> 데이터 분할 

# X_train과 y_train을 학습용 (X_TRAIN, y_TRAIN)과 검증용 (X_VAL, y_VAL)로 분할 
# ( , , , ,) : 변수에 담기 

X_TRAIN, X_VAL, y_TRAIN, y_VAL = train_test_split(X_train, y_train, random_state = 1234, test_size = 0.1) 


# 분할 후 shpae 확인 

print(X_TRAIN.shape)
print(X_VAL.shape)
print(y_TRAIN.shape)
print(y_VAL.shape)



# 인코딩 - 원-핫 인코딩 수행 
# 카테고리형 컬럼(object)에 대하여 원핫인코딩 수행 
# 원핫인코딩 : 범주형 데이터를 숫자로 변환하여, 딥러닝 모델에 적용 가능하게 변환함 
# (ex) 개 1000, 고양이 0100, 원숭이 0010, 돼지 0001 

from sklearn.preprocessing import OneHotEncoder 

# 인코딩 할 카테고리형(object) 컬럼만 별도 저장   

X_TRAIN_category = X_TRAIN.select_dtypes('object').copy() 
X_VAL_category = X_VAL.select_dtypes('object').copy()
X_TEST_category = X_test.select_dtypes('object').copy() 



# 원-핫 인코딩 
# 디폴트는 sparse = True이며 매트릭스 리턴 , sparse = False이면 배열 리턴 
# fit 메서드: 모델을 학습시킬 때 사용함. // 머신러닝이 데이터에 머신러닝 모델을 맞추는 것(fit) 
# transform: fit을 통해 세운 기준(모델)으로 맞춰서 변형하는 함수  

enc = OneHotEncoder(sparse = False).fit(X_TRAIN_category)

X_TRAIN_OH = enc.transform(X_TRAIN_category)
X_VAL_OH = enc.transform(X_VAL_category) 
X_TEST_OH = enc.transform(X_TEST_category) 



# >>> 스케일링 
# 스케일링 ;; 카테고리형이 아닌 것, 숫자 관련된 것. 수치형. 
# StandardScaler(): 표준화. 평균이 0이고 분산 1인 정규분포 만드는 것. z-score  

from sklearn.preprocessing import StandardScaler 

# 스케일링할 컬럼만 별도 저장 
# select_dtypes() 메서드의 exclude 옵션은 해당 dtype을 제외한 모든 dtypes를 추출할 때 사용 
# 테스트는 스플릿이 안되어 있으므로 정의변수는 대문자, 할당 시에는 소문자 사용 

X_TRAIN_conti = X_TRAIN.select_dtypes(exclude = 'object').copy() 
X_VAL_conti = X_VAL.select_dtypes(exclude = 'object').copy() 
X_TEST_conti = X_test.select_dtypes(exclude = 'object').copy() 


# TRAIN 데이터 기준으로 스케일링 함 
scale = StandardScaler().fit(X_TRAIN_conti) 


# Z-점수 표준화 
# ; transform에서 평균, 분산, 표준편차 등 가지고, 거기에 때리면서 구하면서 표준화 
# transform: fit을 통해 세운 기준(모델)으로 맞춰서 변형하는 함수  

X_TRAIN_STD = scale.transform(X_TRAIN_conti)
X_VAL_STD = scale.transform(X_VAL_conti)
X_TEST_STD = scale.transform(X_TEST_conti) 



# >>> 입력 데이터셋 준비  

import numpy as np 

# 인코딩과 스케일링 된 넘파이 배열 연결  

X_TRAIN = np.concatenate([X_TRAIN_OH, X_TRAIN_STD],axis = 1) 
X_VAL = np.concatenate([X_VAL_OH, X_VAL_STD],axis = 1) 


# y를 1차원 넘파이 배열로 평탄화 
# ravel: 1차원 배열로 평평하게 펴주는 함수 

# (참고) 
# 딕셔너리는 key(), values() 
# 그 중 키만 뽑을 때 : key()
# 값만 뽑을 때 : values() 
# 키, 값 쌍으로 뽑고 싶을 때 : items() 


# 1차원 넘파이 배열로 평탄화 
y_TRAIN = y_TRAIN.values.ravel() 
y_VAL = y_VAL.values.ravel() 



# >>> 모델 학습 

from sklearn.ensemble import RandomForestClassifier 


 랜덤 포레스트 

rf = RandomForestClassifier(n_estimators = 500, max_depth = 3, min_samples_leaf = 10, max_features = 'sqrt', random_state = 2022) 

model_rf = rf.fit(X_TRAIN, y_TRAIN) 


# 성능평가(기준:f1-score) 통한 모델 선정 // 단, 여기서는 랜덤포레스트만 사용함 

from sklearn.metrics import f1_score 


# 검증용 데이터셋을 통한 예측 

pred_rf = model_rf.predict(X_VAL) 



# f1-score 계산 

f1_rf = f1_score(y_VAL, pred_rf)

print(f1_rf)



# 결과 제출하기 

X_TEST = np.concatenate([X_TEST_OH, X_TEST_STD],axis = 1)

y_pred = model_rf.predict(X_TEST)


# 문제에서 요구하는 형태로 변환 
# <제출형식> ID , survived 

obj = {'ID':ID, 'survived':y_pred} 

result = pd.DataFrame(obj) 


# 12345.csv로 저장하기
# index = False 시 인덱스를 포함하지 않고 저장 

result.to_csv("12345.csv",index = False) 


================================================
---------------------------------------------------------------------------------------

from sklearn.ensemble import RandomForestClassifier 
from xgboost import XGBClassifier 
from lightgbm import LGBMClassifier


rf = RandomForestClassifier(n_estimators=500, max_depth=3, min_samples_leaf=10, max_features='sqrt', random_state=2022) 
# 모델학습
model_rf = rf.fit(X_TRAIN,y_TRAIN) 


xgb = XGBClassifier(max_depth=8, n_estimators=500, nthread=5, min_child_weight=20, gamma=0.5, objective='binary:logistic', use_label_encoder=False, random_state=2022)
# 모델학습
model_xgb = xgb.fit(X_TRAIN,y_TRAIN,eval_metric='mlogloss')


lgb = LGBMClassifier(max_depth=8, n_estimators=500, n_jobs=30, min_child_weight=10, learning_rate=0.2, objective='binary', random_state=2022)
# 모델학습
model_lgb = lgb.fit(X_TRAIN,y_TRAIN) 



from sklearn.metrics import f1_score   

pred_rf = model_rf.predict(X_VAL) 
pred_xgb = model_xgb.predict(X_VAL) 
pred_lgb = model_lgb.predict(X_VAL) 


f1_rf = f1_score(y_VAL,pred_rf) 
print(f1_rf)


f1_xgb = f1_score(y_VAL, pred_xgb)
print(f1_xgb)


f1_lgb = f1_score(y_VAL, pred_lgb)
print(f1_lgb)

---------------------------------------------------------------------------------------

================================================

[ BigT 2 ]

# 블랙프라이데이 제품 구매자들의 구매 정보에 관련한 데이터의 일부이다. 
# 주어진 데이터를 이용하여 예측 모형을 만들고 CSV 파일 생성 

# (가) 제공 데이터 목록 
# (1) BlackFriday_y_train.csv : 구매자 구매금액 데이터 (학습용), CSV 형식 파일 
# (2) BlackFriday_X_train.csv, BlackFriday_X_test.csv : 구매자 주요 속성(학습용 및 평가용), csv 형식의 파일

# (나) 데이터 형식 및 내용 
# (1)  BlackFriday_y_train.csv (3,900명 데이터, 학습용)
# User_ID : 각 구매자의 고유 ID 
# Purchase : 구매금액(달러) 

# (2) BlackFriday_X_train.csv (3,900명 데이터, 학습용) , BlackFriday_X_test.csv (2,600명 데이터, 평가용)

# 구매자 3,900명에 대한 학습용 데이터 (BlackFriday_y_train.csv, BlackFriday_X_train.csv)를 이용하여 
# 구매금액 예측 모형을 만든 후 이를 평가용 데이터(BlackFriday_X_test.csv)에 적용하여 얻은
# 2,600명 구매자의 구매금액 예측값을 CSV 파일로 생성하시오. 
# (* 제출한 모델의 성능은 "MAE" 평가지표에 따라 채점)

# <제출형식> 
# User_ID : 각 구매자의 고유 ID 
# Purchase : 구매금액(달러) 

# <유의사항> 
# 성능이 우수한 예측모형을 구축하기 위해서는 적절한 데이터 전처리, Feature Engineering, 
# 회귀 알고리즘 사용, 초매개변수 최적화, 모형 앙상블 등이 수반   



import pandas as pd

X_train = pd.read_csv(r'D:\빅데이터\빅데이터분석기사 실기 모의고사\data\모의고사\02회\BlackFriday_X_train.csv')
X_test = pd.read_csv(r'D:\빅데이터\빅데이터분석기사 실기 모의고사\data\모의고사\02회\BlackFriday_X_test.csv')
y_train = pd.read_csv(r'D:\빅데이터\빅데이터분석기사 실기 모의고사\data\모의고사\02회\BlackFriday_y_train.csv')


print(X_train.head())
print(X_test.head())
print(y_train.head())

print(X_train.info())
print(X_test.info())
print(y_train.info())

print(X_train.describe())
print(X_test.describe())
print(y_train.describe())


# 결과 제출 시 X_test의 컬럼이 필요하기 때문에 별도 저장 

User_ID = X_test['User_ID'].copy() 



# >>> 불필요한 컬럼 삭제 

# 별도 저장한 컬럼 및 불필요한 컬럼 삭제 
# Product_ID는 제품 고유 ID로 불필요 하므로 마찬가지로 삭제 

X_train = X_train.drop(columns = ['User_ID','Product_ID'])
X_test = X_test.drop(columns = ['User_ID','Product_ID'])
y_train = y_train.drop(columns = ['User_ID']) 



# >>> 결측치 처리 

# 결측치 확인 

X_train.isna().sum() 

X_test.isna().sum() 


# Product_Category_2는 Product_Category_1의 하위 카테고리 // Product_Category_3은 Product_Category_1,2의 하위 카테고리
# Product_Category_2 컬럼 결측치 다수 존재. train에서 1205 (31%), test에서 848 (33%) 결측치  
# 컬럼을 삭제 

X_train = X_train.drop('Product_Category_2', axis = 1) 
X_test = X_test.drop('Product_Category_2', axis = 1) 


# Product_Category_3 컬럼 결측치 다수 존재. train에서 2687 (69%), test에서 1807 (70%) 결측치  
# 컬럼을 삭제 

X_train = X_train.drop('Product_Category_3', axis = 1) 
X_test = X_test.drop('Product_Category_3', axis = 1) 


# >> 카테고리형 컬럼 전처리 
# 문자열(object) 컬럼들의 유일값 수 확인  

print(X_train.select_dtypes('object').nunique()) 


print(X_test.select_dtypes('object').nunique()) 

# 컬럼별 카테고리 확인 결과 큰 이상 없음 



# >>> 수치형 컬럼 전처리 

X_train.select_dtypes(exclude = 'object') 

X_test.select_dtypes(exclude = 'object') 


# Occupation, Marital_Status, Product_Category_1 컬럼 
# 수치형으로 인식되지만, 카테고리의 의미를 가지는 컬럼 (즉, 문자가 문자가 아닌 숫자 형태로 되어있음) 
# dtype 변경 후 파생변수 xxxxxx_gp에 할당하고 기존 컬럼 삭제 

X_train['OCC_gp'] = X_train['Occupation'].astype('object') 
X_test['OCC_gp'] = X_test['Occupation'].astype('object') 

X_train['Marital_gp'] = X_train['Marital_Status'].astype('object') 
X_test['Marital_gp'] = X_test['Marital_Status'].astype('object') 

X_train['PC_gp'] = X_train['Product_Category_1'].astype('object') 
X_test['PC_gp'] = X_test['Product_Category_1'].astype('object') 


# 데이터 타입 변경한 것 변수에 담기를 완료 후 기존 것 삭제 
# 괄호 안에 대괄호 쓰고 그 안에 ([컬럼1,컬럼2,컬럼3], axis=1)로 하면 한꺼번에 삭제 가능

X_train = X_train.drop(['Occupation', 'Marital_Status', 'Product_Category_1'], axis = 1)
X_test = X_test.drop(['Occupation', 'Marital_Status', 'Product_Category_1'], axis = 1) 


!pip install scikit-learn 

# 사이킷런이 없어서 인스톨 함 


# >>> 데이터 분할 

# X_train과 y_train을 학습용 (X_TRAIN, y_TRAIN)과 검증용 (X_VAL, y_VAL)로 분할 
# ( , , , ,) : 변수에 담기 

from sklearn.model_selection import train_test_split 


X_TRAIN, X_VAL, y_TRAIN,  y_VAL = train_test_split(X_train, y_train, random_state=1234, test_size=0.3) 


# 분할 후 shpae 확인 

print(X_TRAIN.shape) 

print(X_VAL.shape)

print(y_TRAIN.shape)

print(y_VAL.shape)


# 인코딩 - 원-핫 인코딩 수행 
# 카테고리형 컬럼(object)에 대하여 원핫인코딩 수행 
# 원핫인코딩 : 범주형 데이터를 숫자로 변환하여, 딥러닝 모델에 적용 가능하게 변환함 
# (ex) 개 1000, 고양이 0100, 원숭이 0010, 돼지 0001 

from sklearn.preprocessing import OneHotEncoder 


# 인코딩 할 카테고리형(object) 컬럼만 별도 저장   
# 테스트는 스플릿이 안되어 있으므로 정의변수는 대문자, 할당 시에는 소문자 사용 

X_TRAIN_category = X_TRAIN.select_dtypes('object').copy() 
X_VAL_category = X_VAL.select_dtypes('object').copy()
X_TEST_category = X_test.select_dtypes('object').copy() 


# 원-핫 인코딩 
# 디폴트는 sparse = True이며 매트릭스 리턴 , sparse = False이면 배열 리턴 
# fit 메서드: 모델을 학습시킬 때 사용함. // 머신러닝이 데이터에 머신러닝 모델을 맞추는 것(fit) 
# transform: fit을 통해 세운 기준(모델)으로 맞춰서 변형하는 함수 

# X트레인카테고리 정의변수 활용하여 fit 기반 모델 학습  
# 스팔스_아웃풋 = 폴스로 입력함 

enc = OneHotEncoder(sparse_output = False).fit(X_TRAIN_category) 


# X트레인, X밸, X테스트에 대해서 원핫인코딩 함수 기반 트랜스폼 진행 

X_TRAIN_OH = enc.transform(X_TRAIN_category)
X_VAL_OH = enc.transform(X_VAL_category) 
X_TEST_OH = enc.transform(X_TEST_category) 


# >>> 스케일링 
# 스케일링 ;; 카테고리형이 아닌 것, 숫자 관련된 것. 수치형. 
# StandardScaler(): 표준화. 평균이 0이고 분산 1인 정규분포 만드는 것. z-score  

# 여기서는 스케일링 할 수치형 컬럼이 없으므로 생략함! 


# >>> 입력 데이터셋 준비  

import numpy as np 


# 인코딩과 스케일링 된 넘파이 배열 연결  

# 본래는 원핫인코딩한 정의변수와 스케일링한 정의변수를 이어붙이지만, 
# X_TRAIN = np.concatenate([X_TRAIN_OH, X_TRAIN_STD],axis = 1) 
# X_VAL = np.concatenate([X_VAL_OH, X_VAL_STD],axis = 1) 

# 여기서는 스케일링 생략했으므로 원핫인코딩 정의변수에다 할당 후 
# 1차원 넘파이 배열 평탄화 바로 진행 

X_TRAIN = X_TRAIN_OH 
X_VAL = X_VAL_OH  


# y를 1차원 넘파이 배열로 평탄화 
# ravel: 1차원 배열로 평평하게 펴주는 함수 

# (참고) 
# 딕셔너리는 key(), values() 
# 그 중 키만 뽑을 때 : key()
# 값만 뽑을 때 : values() 
# 키, 값 쌍으로 뽑고 싶을 때 : items() 

# 1차원 넘파이 배열로 평탄화 
y_TRAIN = y_TRAIN.values.ravel() 
y_VAL = y_VAL.values.ravel() 


# >>> 모델 학습

from sklearn.ensemble import RandomForestRegressor 


rf = RandomForestRegressor(n_estimators=500, max_depth=3, min_samples_leaf=10, max_features=2, random_state=2022) 


# 랜덤 포레스트 fit 진행 시 X_대문자 트레인, y_대문자 트레인으로 fit 진행 

model_rf = rf.fit(X_TRAIN, y_TRAIN) 


# 성능평가(기준:MAE) 통한 모델 선정 // 단, 여기서는 랜덤포레스트만 사용함 

from sklearn.metrics import mean_absolute_error 


# 검증용 데이터셋을 통한 예측 
# 프레드알에프는, 모델알에프 쩜 프레딕트 (X_밸)

pred_rf = model_rf.predict(X_VAL) 


# MAE 계산 

# 엠애이알에프는, m_a_e(y_밸, 프레드알에프(여기에 X_밸 내재)) 

mae_rf = mean_absolute_error(y_VAL, pred_rf)


print(mae_rf) 

# 참고로, 리그레션은 낮을수록 좋다. 


# 결과 제출하기  

# 본래는 아래처럼 이어 붙여서 진행 
# X_TEST = np.concatenate([X_TEST_OH, X_TEST_STD],axis = 1)
# y_pred = model_rf.predict(X_TEST) 

X_TEST = X_TEST_OH 
y_pred = model_rf.predict(X_TEST) 


# 문제에서 요구하는 형태로 변환 
# <제출형식> User_ID , Purchase 

obj = {'User_ID':User_ID, 'Purchase':y_pred} 

result = pd.DataFrame(obj) 


# 12345.csv로 저장하기
# index = False 시 인덱스를 포함하지 않고 저장 

result.to_csv("12345.csv",index = False) 


================================================

[ Big T 3 ] 

# 아래는 호주의 기상 관측소들의 일자별 기상 정보와 강수 여부에 관련한 데이터의 일부이다. 
# 주어진 데이터를 이용하여 예측 모형을 만들고 아래에 따라 CSV 파일을 생성하시오. 

# (가) 제공 데이터 목록 
   # (1) weatherAUS_y_train.csv : 일자별 강수 여부 데이터(학습용), csv 형식의 파일 
   # (2) weatherAUS_x_train.csv , weatherAUS_x_test.csv : 일자별 호주 기상 데이터(학습용 및 평가용), csv 형식의 파일 

# (나) 데이터 형식 및 내용 
   # (1) weatherAUS_y_train.csv (11,714 데이터)   
       # Date : 날짜 // RainTommorow : 강수 여부 ("No": 오지 않음 , "Yes": 옴) 
   # (2) weatherAUS_x_train.csv (11,714 데이터) , weatherAUS_x_test.csv (5,794 데이터)  

# 11,714일간의 학습용 데이터(weatherAUS_y_train.csv , weatherAUS_x_train.csv)를 이용하여, 기상 여부 예측 모형을 만든 후  
# 이를 평가용 데이터(weatherAUS_x_test.csv)에 적용하여 얻은 5,794일의 기상 여부 예측값을 CSV 파일로 생성하시오 
# (제출한 모델의 성능은 AUC 평가지표에 따라 채점) 

# <제출형식> 
# Date : 날짜 // RainTommorow_Prob : 강수 여부 예측 확률 

# <유의사항>
# 성능이 우수한 예측모형을 구축하기 위해서 
# 적절한 데이터 전처리 , Feature Engineering , 분류 알고리즘 사용 , 초매개변수 최적화 , 모형 앙상블 등 수반
# 시계열성을 고려하지 않는다.  



import pandas as pd  

X_train = pd.read_csv(r'C:\빅데이터분석기사 실기 모의고사\data\모의고사\03회/weatherAUS_x_train.csv')
X_test = pd.read_csv(r'C:\빅데이터분석기사 실기 모의고사\data\모의고사\03회/weatherAUS_x_test.csv')
y_train = pd.read_csv(r'C:\빅데이터분석기사 실기 모의고사\data\모의고사\03회/weatherAUS_y_train.csv')


print(X_train.head())
print(X_test.head())
print(y_train.head())


print(X_train.info())
print(X_test.info())
print(y_train.info())


print(X_train.describe())
print(X_test.describe())
print(y_train.describe()) 



# 결과 제출 시 X_test의 컬럼이 필요하기 때문에 별도 저장 

Date = X_test['Date'].copy()


# 별도 저장한 컬럼 및 불필요한 컬럼 삭제 
X_train = X_train.drop(columns = 'Date' )
X_test = X_test.drop(columns = ['Date'])
y_train = y_train.drop(columns = 'Date') 



# >>> 결측치 처리 

# 결측치 확인 
X_train.isna().sum()

X_test.isna().sum() 

# 교재풀이 때문에 확인해 봄
y_train.isna().sum()


# train에서 500개가 넘는 결측치가 있는 컬럼은 삭제 

# 결측치가 500개가 넘는 조건 
cond_na500 = (X_train.isna().sum() >=500)


# 500개가 넘는 컬럼명 
col_na500 = X_train.columns[cond_na500]   


# (모은) 컬럼 삭제 
X_train = X_train.drop(col_na500, axis =1)
X_test = X_test.drop(col_na500, axis =1)



# train에서 100개 미만의 결측치가 있는 컬럼은 결측치 대체 

# 결측치가 100개 미만의 조건 
X_train.isna().sum() < 100 
; (실행해보면 컬럼들 다 true로 나옴)


# 수치형인 컬럼들은 평균 대체 (MinTemp, MaxTemp, Rainfall, WindSpeed9am, Humidity9am, Temp9am) 

# 수치형만 있는 데이터프레임 추출 
X_train_conti = X_train.select_dtypes(exclude = 'object').copy() 
X_test_conti = X_test.select_dtypes(exclude = 'object').copy() 


# 평균 대치 
X_train_conti = X_train_conti.fillna(X_train_conti.mean()) 
X_test_conti = X_test_conti.fillna(X_test_conti.mean()) 



# 카테고리형인 컬럼은 최다빈도를 가지는 라벨로 대체 (RainToday) 

# 카테고리형만 있는 데이터프레임 추출 
X_train_category = X_train.select_dtypes('object').copy() 
X_test_category = X_test.select_dtypes('object').copy()  


# 최다라벨로 대체 
mode = X_train_category.value_counts('RainToday').idxmax()  


X_train_category = X_train_category.fillna(mode)
X_test_category = X_test_category.fillna(mode) 



# 두 데이터 프레임 다시 합치기 (수치형 작업한 것 + 카테고리형 작업한 것) 

X_train = pd.concat([X_train_conti, X_train_category], axis=1) 
X_test = pd.concat([X_test_conti,X_test_category], axis=1)  


# 카테고리형 컬럼 전처리 : 별도 과정 불필요로 생략 

# 상태 확인해보기 
print(X_train.select_dtypes('object').nunique())


print(X_test.select_dtypes('object').nunique())

# 수치형 컬럼 전처리 : 별도 과정 불필요로 생략 



# >>> 데이터 분할 

# X_train과 y_train을 학습용 (X_TRAIN, y_TRAIN)과 검증용 (X_VAL, y_VAL)로 분할 
# ( , , , ,) : 변수에 담기 

from sklearn.model_selection import train_test_split  


# X_train과 y_train을 학습용 (X_TRAIN, y_TRAIN)과 검증용 (X_VAL, y_VAL)로 분할 

X_TRAIN, X_VAL, y_TRAIN, y_VAL = train_test_split(X_train, y_train, random_state=1234, test_size=0.3) 



# 분할 후 shpae 확인 

print(X_TRAIN.shape)

print(X_VAL.shape)

print(y_TRAIN.shape)

print(y_VAL.shape)



# 인코딩 - 원-핫 인코딩 수행 
# 카테고리형 컬럼(object)에 대하여 원핫인코딩 수행 
# 원핫인코딩 : 범주형 데이터를 숫자로 변환하여, 딥러닝 모델에 적용 가능하게 변환함 
# (ex) 개 1000, 고양이 0100, 원숭이 0010, 돼지 0001 

from sklearn.preprocessing import OneHotEncoder 


# 인코딩 할 카테고리형(object) 컬럼만 별도 저장   
# 트레인은 스플릿을 했지만 테스트는 안했으므로 소문자로 사용 

X_TRAIN_category = X_TRAIN.select_dtypes('object').copy() 
X_VAL_category = X_VAL.select_dtypes('object').copy() 
X_TEST_category = X_test.select_dtypes('object').copy() 



# 원-핫 인코딩 
# 디폴트는 sparse = True이며 매트릭스 리턴 , sparse = False이면 배열 리턴 
# fit 메서드: 모델을 학습시킬 때 사용함. // 머신러닝이 데이터에 머신러닝 모델을 맞추는 것(fit) 
# transform: fit을 통해 세운 기준(모델)으로 맞춰서 변형하는 함수  

enc = OneHotEncoder(sparse = False).fit(X_TRAIN_category)


X_TRAIN_OH = enc.transform(X_TRAIN_category)
X_VAL_OH = enc.transform(X_VAL_category) 
X_TEST_OH = enc.transform(X_TEST_category) 



# >>> 스케일링 
# 스케일링 ;; 카테고리형이 아닌 것, 숫자 관련된 것. 수치형. 
# StandardScaler(): 표준화. 평균이 0이고 분산 1인 정규분포 만드는 것. z-score  

from sklearn.preprocessing import StandardScaler 


# 스케일링할 컬럼만 별도 저장 
# select_dtypes() 메서드의 exclude 옵션은 해당 dtype을 제외한 모든 dtypes를 추출할 때 사용 
# 테스트는 스플릿이 안되어 있으므로 정의변수는 대문자, 할당 시에는 소문자 사용 

X_TRAIN_conti = X_TRAIN.select_dtypes(exclude = 'object').copy() 
X_VAL_conti = X_VAL.select_dtypes(exclude = 'object').copy() 
X_TEST_conti = X_test.select_dtypes(exclude = 'object').copy() 


# TRAIN 데이터 기준으로 스케일링 함 
scale = StandardScaler().fit(X_TRAIN_conti) 


# Z-점수 표준화 
# ; transform에서 평균, 분산, 표준편차 등 가지고, 거기에 때리면서 구하면서 표준화 
# transform: fit을 통해 세운 기준(모델)으로 맞춰서 변형하는 함수  

X_TRAIN_STD = scale.transform(X_TRAIN_conti)
X_VAL_STD = scale.transform(X_VAL_conti)
X_TEST_STD = scale.transform(X_TEST_conti) 



# >>> 입력 데이터셋 준비  

import numpy as np 


# 인코딩과 스케일링 된 넘파이 배열 연결  

X_TRAIN = np.concatenate([X_TRAIN_OH, X_TRAIN_STD],axis = 1) 
X_VAL = np.concatenate([X_VAL_OH, X_VAL_STD],axis = 1) 


# 'Yes'와 'No'를 각각 1,0에 매핑 (yes와 no는 y_train 내에 있던 데이터) // (여기 작업예시 특징) 

y_TRAIN = y_TRAIN['RainTomorrow'].map({'Yes':1, 'No':0})
y_VAL = y_VAL['RainTomorrow'].map({'Yes':1, 'No':0}) 


# y를 1차원 넘파이 배열로 평탄화 
# ravel: 1차원 배열로 평평하게 펴주는 함수 

# (참고) 
# 딕셔너리는 key(), values() 
# 그 중 키만 뽑을 때 : key()
# 값만 뽑을 때 : values() 
# 키, 값 쌍으로 뽑고 싶을 때 : items() 

# 1차원 넘파이 배열로 평탄화 
y_TRAIN = y_TRAIN.values.ravel() 
y_VAL = y_VAL.values.ravel() 



# >>> 모델 학습 

from sklearn.ensemble import RandomForestClassifier 


# 랜덤 포레스트 

rf = RandomForestClassifier(n_estimators=500, max_depth=3, min_samples_leaf=10, max_features='sqrt', random_state=2022) 


model_rf = rf.fit(X_TRAIN, y_TRAIN)

# fit 메서드: 모델을 학습시킬 때 사용함. // 머신러닝이 데이터에 머신러닝 모델을 맞추는 것(fit) 


# 성능평가(기준:AUC) 통한 모델 선정 // 단, 여기서는 랜덤포레스트만 사용함 

from sklearn.metrics import roc_curve, auc 


# 검증용 데이터셋을 통한 예측 
# [:,1]는; 모든 행에 대해서, 두번째 열의 정보를 가져다 달라  

score_rf = model_rf.predict_proba(X_VAL)[:,1] 



## AUC 계산 

# roc_curve 함수의 결과를 fpr, tpr, thresholds 변수로 받아주는 것 
# fpr: false positive rate // tpr: true positive rate // thresholds: 임계값, 애매한 값을 분류할 기준이 필요, 이 기준을 임계값이라 함

fpr, tpr, thresholds = roc_curve(y_VAL, score_rf) 


auc_rf = auc(fpr, tpr) 

# auc 값은 roc 곡선 밑 면적을 구한 것 // 1에  가까울 수록 좋은 수치, 0.5에 가까울수록 학습이 제대로 이루어지지 않은 모델 
# AUC = Area Under Roc curve // ROC커브 = Receiver Operating Characteristic 

print(auc_rf) 



# 결과 제출하기 

X_TEST = np.concatenate([X_TEST_OH, X_TEST_STD],axis = 1)

y_score = model_rf.predict_proba(X_TEST)[:,1] 


# 문제에서 요구하는 형태로 변환 
# <제출형식> Date , RainTomorrow_prob 

obj = {'Date':Date, 'RainTomorrow_prob':y_score} 

result = pd.DataFrame(obj) 


# 12345.csv로 저장하기
# index = False 시 인덱스를 포함하지 않고 저장 

result.to_csv("12345.csv",index = False)  


================================================

[ Big 2nd last test ]

# 뇌졸중 환자들의 임상적 변수에 관련한 데이터의 일부이다.
# 주어진 데이터를 이용하여 예측 모형을 만들고 CSV 파일 생성 

# (가) 제공 데이터 목록 
# (1) stroke_y_train.csv : 뇌졸중 여부 데이터(학습용), csv 형식의 파일 
# (2) stroke_X_train.csv, stroke_X_test.csv : 뇌졸중에 대한 임상적 변수(학습용 및 평가용), csv 형식의 파일

# (나) 데이터 형식 및 내용
# (1) stroke_y_train.csv (4, 087명 데이터)
# id : 각 환자 고유 ID 
# stroke : 환자에게 뇌졸중이 있는지 여부 (0: 없음, 1: 있음) 

# (2) stroke_X_train.csv (4, 087명 데이터), stroke_X_test.csv (1,022명 데이터)

# 환자 4,087명에 대한 학습용 데이터 (stroke_y_train.csv, stroke_X_train.csv)를 이용하여
# 뇌졸중 여부 예측 모형을 만든 후 이를 평가용 데이터(stroke_X_test.csv)에 적용하여 얻은
# 1,022명 환자의 뇌졸중 여부 예측값을 CSV 파일로 생성하시오. 
# (* 제출한 모델의 성능은 "Accuracy" 평가지표에 따라 채점)

# <제출형식> 
#  id : 각 환자 고유 ID 
# stroke : 환자에게 뇌졸중이 있는지 여부 (0: 없음, 1: 있음) 

# <유의사항> 
# 성능이 우수한 예측모형을 구축하기 위해서는 적절한 데이터 전처리, Feature Engineering, 
# "분류" 알고리즘 사용, 초매개변수 최적화, 모형 앙상블 등이 수반   



import pandas as pd

X_train = pd.read_csv(r'D:\빅데이터\빅데이터분석기사 실기 모의고사\data\기출복원\02회\stroke_X_train.csv')
X_test = pd.read_csv(r'D:\빅데이터\빅데이터분석기사 실기 모의고사\data\기출복원\02회\stroke_X_test.csv')
y_train = pd.read_csv(r'D:\빅데이터\빅데이터분석기사 실기 모의고사\data\기출복원\02회\stroke_y_train.csv') 


print(X_train.head())
print(X_test.head())
print(y_train.head())

print(X_train.info())
print(X_test.info())
print(y_train.info())

print(X_train.describe())
print(X_test.describe())
print(y_train.describe())



# >>> 불필요한 컬럼 삭제 

# 결과 제출 시 X_test의 컬럼이 필요하기 때문에 별도 저장 

ID = X_test['id'].copy() 


# 컬럼 별도 저장 완료 후 불필요한 컬럼 삭제 

X_train = X_train.drop(columns = 'id')
X_test = X_test.drop(columns = 'id')
y_train = y_train.drop(columns = 'id') 



# >>> 결측치 처리 

# 결측치 확인 

X_train.isna().sum() 

X_test.isna().sum() 



# smoking_status 내 Unknown은 정보에 대해 알 수 없는 것으로 추정됨 // 문제에서 알려 줌 

X_train['smoking_status'].value_counts()  

X_test['smoking_status'].value_counts()  



# bmi 컬럼 (train 165, test 36 결측)

# 평균값 대치 
  
avg_bmi = X_train['bmi'].mean()  

X_train['bmi'] = X_train['bmi'].fillna(avg_bmi) 
X_test['bmi'] = X_test['bmi'].fillna(avg_bmi)  


# smoking_status 컬럼 (train 1,240, test 304 결측) 

# 컬럼을 삭제 

X_train = X_train.drop('smoking_status', axis = 1) 
X_test = X_test.drop('smoking_status', axis = 1) 



# >> 카테고리형 컬럼 전처리 
# 문자열(object) 컬럼들의 유일값 수 확인 

print(X_train.select_dtypes('object').nunique()) 


print(X_test.select_dtypes('object').nunique()) 

# 별도 이상이 없으므로 키테고리형 컬럼 전처리는 생략 



# >>> 수치형 컬럼 전처리 

# age 컬럼 
# 구간화(binning)하여 age_gp에 할당 - dtype을 object형으로 변경 
# range(A): 0부터 A-1까지 반환 
# range(A,B): A부터 B-1까지 반환
# range(A,B,C): A부터 B-1까지 C간격으로 반환

X_train['age_gp'] = pd.cut(X_train['age'], bins=list(range(0,100,10))).astype('object')
X_test['age_gp'] = pd.cut(X_test['age'], bins=list(range(0,100,10))).astype('object')


# 완료 후 기존 컬럼 삭제 

X_train = X_train.drop('age', axis = 1)
X_test = X_test.drop('age', axis = 1) 


# >>> 데이터 분할 

# X_train과 y_train을 학습용 (X_TRAIN, y_TRAIN)과 검증용 (X_VAL, y_VAL)로 분할 
# ( , , , ,) : 변수에 담기 
# 검증용 VAL은 나중에 성능평가 시에 검증용 데이터를 통한 예측을 할 때 사용함 


from sklearn.model_selection import train_test_split 


X_TRAIN, X_VAL, y_TRAIN, y_VAL = train_test_split(X_train, y_train, random_state=1234, test_size=0.3) 


# 분할 후 shpae 확인 

print(X_TRAIN.shape) 


print(X_VAL.shape)

print(y_TRAIN.shape)

print(y_VAL.shape)



# 인코딩 - 원-핫 인코딩 수행 
# 카테고리형 컬럼(object)에 대하여 원핫인코딩 수행 
# 원핫인코딩 : 범주형 데이터를 숫자로 변환하여, 딥러닝 모델에 적용 가능하게 변환함 
# (ex) 개 1000, 고양이 0100, 원숭이 0010, 돼지 0001 

from sklearn.preprocessing import OneHotEncoder 


# 인코딩 할 카테고리형(object) 컬럼만 별도 저장   
# 테스트는 스플릿이 안되어 있으므로 정의변수는 대문자, 할당 시에는 소문자 사용 

X_TRAIN_category = X_TRAIN.select_dtypes('object').copy() 
X_VAL_category = X_VAL.select_dtypes('object').copy()
X_TEST_category = X_test.select_dtypes('object').copy() 


# 원-핫 인코딩 
# 디폴트는 sparse = True이며 매트릭스 리턴 , sparse = False이면 배열 리턴 
# fit 메서드: 모델을 학습시킬 때 사용함. // 머신러닝이 데이터에 머신러닝 모델을 맞추는 것(fit) 
# transform: fit을 통해 세운 기준(모델)으로 맞춰서 변형하는 함수 

# X트레인카테고리 정의변수 활용하여 fit 기반 모델 학습  
# 스팔스_아웃풋 = 폴스로 입력함 

enc = OneHotEncoder(sparse_output = False).fit(X_TRAIN_category)  


# X트레인, X밸, X테스트에 대해서 원핫인코딩 함수 기반 트랜스폼 진행 

X_TRAIN_OH = enc.transform(X_TRAIN_category)
X_VAL_OH = enc.transform(X_VAL_category) 
X_TEST_OH = enc.transform(X_TEST_category) 



# >>> 스케일링 
# 스케일링 ;; 카테고리형이 아닌 것, 숫자 관련된 것. 수치형. 
# StandardScaler(): 표준화. 평균이 0이고 분산 1인 정규분포 만드는 것. z-score  

from sklearn.preprocessing import StandardScaler 


# 스케일링할 컬럼만 별도 저장 
# select_dtypes() 메서드의 exclude 옵션은 해당 dtype을 제외한 모든 dtypes를 추출할 때 사용 
# 테스트는 스플릿이 안되어 있으므로 정의변수는 대문자, 할당 시에는 소문자 사용 

X_TRAIN_conti = X_TRAIN.select_dtypes(exclude = 'object').copy() 
X_VAL_conti = X_VAL.select_dtypes(exclude = 'object').copy() 
X_TEST_conti = X_test.select_dtypes(exclude = 'object').copy() 


# TRAIN 데이터 기준으로 스케일링 함 
# 스탠다드스케일러에 괄호 포함 

scale = StandardScaler().fit(X_TRAIN_conti) 



# Z-점수 표준화 
# ; transform에서 평균, 분산, 표준편차 등 가지고, 거기에 때리면서 구하면서 표준화 
# transform: fit을 통해 세운 기준(모델)으로 맞춰서 변형하는 함수  

X_TRAIN_STD = scale.transform(X_TRAIN_conti)
X_VAL_STD = scale.transform(X_VAL_conti)
X_TEST_STD = scale.transform(X_TEST_conti) 



# >>> 입력 데이터셋 준비 

import numpy as np 


# 인코딩과 스케일링 된 넘파이 배열 연결  

X_TRAIN = np.concatenate([X_TRAIN_OH, X_TRAIN_STD], axis = 1) 
X_VAL = np.concatenate([X_VAL_OH, X_VAL_STD], axis = 1) 


# y를 1차원 넘파이 배열로 평탄화 
# ravel: 1차원 배열로 평평하게 펴주는 함수 

# (참고) 
# 딕셔너리는 key(), values() 
# 그 중 키만 뽑을 때 : key()
# 값만 뽑을 때 : values() 
# 키, 값 쌍으로 뽑고 싶을 때 : items() 

# 1차원 넘파이 배열로 평탄화 
y_TRAIN = y_TRAIN.values.ravel() 
y_VAL = y_VAL.values.ravel() 


# >>> 모델 학습

from sklearn.ensemble import RandomForestClassifier 


# 랜덤 포레스트 

rf = RandomForestClassifier(n_estimators=500, max_depth=3, min_samples_leaf=10, max_features=2, random_state=2022) 


model_rf = rf.fit(X_TRAIN, y_TRAIN)

# fit 메서드: 모델을 학습시킬 때 사용함. // 머신러닝이 데이터에 머신러닝 모델을 맞추는 것(fit) 


# 성능평가(기준:accuracy_score) 통한 모델 선정 // 단, 여기서는 랜덤포레스트만 사용함 

from sklearn.metrics import accuracy_score 


# 검증용 데이터셋을 통한 예측 
# 프레드알에프는, 모델알에프 쩜 프레딕트 (X_밸)

pred_rf = model_rf.predict(X_VAL) 



## accuracy 계산 

# 엠애이알에프는, m_a_e(y_밸, 프레드알에프(여기에 X_밸 내재)) 

acc_rf = accuracy_score(y_VAL, pred_rf) 


print(acc_rf) 



# 결과 제출하기 
# ((참고) [:,1]는, '모든 행에 대해서 두번째 열의 정보를 가져다 달라') 

X_TEST = np.concatenate([X_TEST_OH, X_TEST_STD],axis = 1)

y_score = model_rf.predict_proba(X_TEST)[:,1] 


# 문제에서 요구하는 형태로 변환 
# <제출형식> id , stroke 


obj = {'id':ID, 'stroke':y_score} 

result = pd.DataFrame(obj) 


# 12345.csv로 저장하기
# index = False 시 인덱스를 포함하지 않고 저장 

result.to_csv("12345.csv",index = False)  


================================================

[ Big 3rd last test ]

# HR 연구를 위한 이직 희망 여부와 입사 지원자들의 정보와 관련한 데이터의 일부이다. 
# 주어진 데이터를 이용하여 예측 모형을 만들고 CSV 파일 생성 
# (단, 제출 전 두 개 이상의 모형의 성능을 비교하여 가장 우수한 모형을 선정할 것) 

# (가) 제공 데이터 목록 
# (1) job_change_y_train.csv : 이직 희망 여부 데이터(학습용), csv 형식의 파일  
# (2) job_change_X_train.csv, job_change_X_test.csv : 입사 지원자들의 정보 데이터(학습용 및 평가용), csv 형식의 파일

# (나) 데이터 형식 및 내용
# (1) job_change_y_train.csv (10,063명 데이터) 
# enrollee_id : 지원자의 고유 id 
# target  : 이직 희망 여부 (0: 희망하지 않음 , 1:희망함) 

# (2) job_change_X_train.csv (10,063명 데이터), job_change_X_test.csv (4,313명 데이터)

# 10,063명에 대한 학습용 데이터 (job_change_y_train.csv, job_change_X_train.csv)를 이용하여
# 이직 희망 여부 예측 모형을 만든 후 이를 평가용 데이터( job_change_X_test.csv)에 적용하여 얻은
# 4,313명의 이직 희망 여부 예측 확률을 CSV 파일로 생성하시오. 
# (* 제출한 모델의 성능은 "AUC" 평가지표에 따라 채점)

# <제출형식> 
# enrollee_id : 지원자의 고유 id 
# target  : 이직 희망 여부 (0: 희망하지 않음 , 1:희망함) 
# target_prob : 이직 희망 예측 확률 

# <유의사항> 
# 성능이 우수한 예측모형을 구축하기 위해서는 적절한 데이터 전처리, Feature Engineering, 
# "분류" 알고리즘 사용, 초매개변수 최적화, 모형 앙상블 등이 수반 



import pandas as pd

X_train = pd.read_csv(r'D:\빅데이터\빅데이터분석기사 실기 모의고사\data\기출복원\03회\job_change_X_train.csv')
X_test = pd.read_csv(r'D:\빅데이터\빅데이터분석기사 실기 모의고사\data\기출복원\03회\job_change_y_train.csv')
y_train = pd.read_csv(r'D:\빅데이터\빅데이터분석기사 실기 모의고사\data\기출복원\03회\job_change_X_test.csv') 


print(X_train.head())
print(X_test.head())
print(y_train.head())

print(X_train.info())
print(X_test.info())
print(y_train.info())

print(X_train.describe())
print(X_test.describe())
print(y_train.describe())



# >>> 불필요한 컬럼 삭제 

# 결과 제출 시 X_test의 컬럼이 필요하기 때문에 별도 저장 

enrollee_id = X_test['enrollee_id'].copy() 


# 컬럼 별도 저장 완료 후 불필요한 컬럼 삭제 

X_train = X_train.drop(columns = 'enrollee_id')
X_test = X_test.drop(columns = 'enrollee_id')
y_train = y_train.drop(columns = 'enrollee_id') 



# >>> 결측치 처리 

# 결측치 확인 

X_train.isna().sum() 

X_test.isna().sum() 


# 교재풀이 때문에 확인해 봄
y_train.isna().sum()



# train에서 1,000개가 넘는 결측치가 있는 컬럼은 삭제 

# 결측치가 1,000개가 넘는 조건 
cond_na1000 = (X_train.isna().sum() > 1000)


# 1,000개가 넘는 컬럼명 

col_na1000 = X_train.columns[cond_na1000]   


# (모은) 컬럼 삭제 

X_train = X_train.drop(col_na1000, axis =1)
X_test = X_test.drop(col_na1000, axis =1)


# train에서 200개 미만의 결측치가 있는 컬럼은 결측치 대체 (enrolled_university , education_level)
# valuecounts : 열에 있는 모든 고유값의 개수를 반환함 


# enrolled_university 컬럼 (train 142, test 51 결측) 

# 최다빈도를 가지는 라벨로 대체  

mode_EU = X_train['enrolled_university'].value_counts().idxmax()


X_train['enrolled_university'] = X_train['enrolled_university'].fillna(mode_EU) 
X_test['enrolled_university'] = X_test['enrolled_university'].fillna(mode_EU)


#  education_level 컬럼 (train 162, test 82 결측) 

# 최다빈도를 가지는 라벨로 대체  

mode_EL = X_train['education_level'].value_counts().idxmax() 


X_train['education_level'] = X_train['education_level'].fillna(mode_EL) 
X_test['education_level'] = X_test['education_level'].fillna(mode_EL) 



# >> 카테고리형 컬럼 전처리 
# 문자열(object) 컬럼들의 유일값 수 확인 

print(X_train.select_dtypes('object').nunique()) 


print(X_test.select_dtypes('object').nunique()) 

# 별도 이상이 없으므로 키테고리형 컬럼 전처리는 생략 



# >>> 수치형 컬럼 전처리 

# 수치형으로 인식되지만, 카테고리의 의미를 가지는 컬럼 (즉, 문자가 문자가 아닌 숫자 형태로 되어있음) 
# dtype 변경 후 파생변수 xxxxx_gp에 할당하고 기존 컬럼 삭제  

# 예시
# X_train['pclass_gp'] = X_train['pclass'].astype('object')
# X_test['pclass_gp'] = X_test['pclass'].astype('object') 
# X_train = X_train.drop('pclass', axis = 1)
# X_test = X_test.drop('pclass', axis = 1) 

# 별도 이상이 없으므로 수치형 컬럼 전처리는 생략  



# >>> 데이터 분할 

# X_train과 y_train을 학습용 (X_TRAIN, y_TRAIN)과 검증용 (X_VAL, y_VAL)로 분할 
# ( , , , ,) : 변수에 담기 
# 검증용 VAL은 나중에 성능평가 시에 검증용 데이터를 통한 예측을 할 때 사용함 

from sklearn.model_selection import train_test_split 


X_TRAIN, X_VAL, y_TRAIN, y_VAL = train_test_split(X_train, y_train, random_state=1234, test_size=0.2) 


# 분할 후 shpae 확인 

print(X_TRAIN.shape) 


print(X_VAL.shape)

print(y_TRAIN.shape)

print(y_VAL.shape)



# 인코딩 - 원-핫 인코딩 수행 
# 카테고리형 컬럼(object)에 대하여 원핫인코딩 수행 
# 원핫인코딩 : 범주형 데이터를 숫자로 변환하여, 딥러닝 모델에 적용 가능하게 변환함 
# (ex) 개 1000, 고양이 0100, 원숭이 0010, 돼지 0001 

from sklearn.preprocessing import OneHotEncoder 


# 인코딩 할 카테고리형(object) 컬럼만 별도 저장   
# 테스트는 스플릿이 안되어 있으므로 정의변수는 대문자, 할당 시에는 소문자 사용 

X_TRAIN_category = X_TRAIN.select_dtypes('object').copy() 
X_VAL_category = X_VAL.select_dtypes('object').copy()
X_TEST_category = X_test.select_dtypes('object').copy() 


# 원-핫 인코딩 
# 디폴트는 sparse = True이며 매트릭스 리턴 , sparse = False이면 배열 리턴 
# fit 메서드: 모델을 학습시킬 때 사용함. // 머신러닝이 데이터에 머신러닝 모델을 맞추는 것(fit) 
# transform: fit을 통해 세운 기준(모델)으로 맞춰서 변형하는 함수 

# X트레인카테고리 정의변수 활용하여 fit 기반 모델 학습  
# 스팔스_아웃풋 = 폴스로 입력함 
# handle_unknown='ignore' : fit 단계에서 배열에 없는 값이 있을 때 넣어주면 ValueError 문구 없이 정상 실행 

enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore').fit(X_TRAIN_category) 


# X트레인, X밸, X테스트에 대해서 원핫인코딩 함수 기반 트랜스폼 진행 

X_TRAIN_OH = enc.transform(X_TRAIN_category)
X_VAL_OH = enc.transform(X_VAL_category) 
X_TEST_OH = enc.transform(X_TEST_category) 



# >>> 스케일링 
# 스케일링 ;; 카테고리형이 아닌 것, 숫자 관련된 것. 수치형. 
# StandardScaler(): 표준화. 평균이 0이고 분산 1인 정규분포 만드는 것. z-score  

from sklearn.preprocessing import StandardScaler 


# 스케일링할 컬럼만 별도 저장 
# select_dtypes() 메서드의 exclude 옵션은 해당 dtype을 제외한 모든 dtypes를 추출할 때 사용 
# 테스트는 스플릿이 안되어 있으므로 정의변수는 대문자, 할당 시에는 소문자 사용 

X_TRAIN_conti = X_TRAIN.select_dtypes(exclude = 'object').copy() 
X_VAL_conti = X_VAL.select_dtypes(exclude = 'object').copy() 
X_TEST_conti = X_test.select_dtypes(exclude = 'object').copy() 


# TRAIN 데이터 기준으로 스케일링 함 
# 스탠다드스케일러에 괄호 포함 

scale = StandardScaler().fit(X_TRAIN_conti) 



# Z-점수 표준화 
# ; transform에서 평균, 분산, 표준편차 등 가지고, 거기에 때리면서 구하면서 표준화 
# transform: fit을 통해 세운 기준(모델)으로 맞춰서 변형하는 함수  

X_TRAIN_STD = scale.transform(X_TRAIN_conti)
X_VAL_STD = scale.transform(X_VAL_conti)
X_TEST_STD = scale.transform(X_TEST_conti) 



# >>> 입력 데이터셋 준비 

import numpy as np 


# 인코딩과 스케일링 된 넘파이 배열 연결  

X_TRAIN = np.concatenate([X_TRAIN_OH, X_TRAIN_STD], axis = 1) 
X_VAL = np.concatenate([X_VAL_OH, X_VAL_STD], axis = 1) 


# y를 1차원 넘파이 배열로 평탄화 
# ravel: 1차원 배열로 평평하게 펴주는 함수 

# (참고) 
# 딕셔너리는 key(), values() 
# 그 중 키만 뽑을 때 : key()
# 값만 뽑을 때 : values() 
# 키, 값 쌍으로 뽑고 싶을 때 : items() 

# 1차원 넘파이 배열로 평탄화 
y_TRAIN = y_TRAIN.values.ravel() 
y_VAL = y_VAL.values.ravel() 



# >>> 모델 학습

from sklearn.ensemble import RandomForestClassifier 


# 랜덤 포레스트 

rf = RandomForestClassifier(n_estimators=500, max_depth=3, min_samples_leaf=10, max_features='sqrt', random_state=2022) 


model_rf = rf.fit(X_TRAIN, y_TRAIN)

# fit 메서드: 모델을 학습시킬 때 사용함. // 머신러닝이 데이터에 머신러닝 모델을 맞추는 것(fit) 


# 성능평가(기준:AUC 모델 선정 // 단, 여기서는 랜덤포레스트만 사용함 

from sklearn.metrics import roc_curve, auc 


# 검증용 데이터셋을 통한 예측 
# [:,1]는; 모든 행에 대해서, 두번째 열의 정보를 가져다 달라  

score_rf = model_rf.predict_proba(X_VAL)[:,1] 


## AUC 계산 

# roc_curve 함수의 결과를 fpr, tpr, thresholds 변수로 받아주는 것 
# fpr: false positive rate // tpr: true positive rate // thresholds: 임계값, 애매한 값을 분류할 기준이 필요, 이 기준을 임계값이라 함

fpr, tpr, thresholds = roc_curve(y_VAL, score_rf) 


auc_rf = auc(fpr, tpr) 

# auc 값은 roc 곡선 밑 면적을 구한 것 // 1에  가까울 수록 좋은 수치, 0.5에 가까울수록 학습이 제대로 이루어지지 않은 모델 
# AUC = Area Under Roc curve // ROC커브 = Receiver Operating Characteristic 


print(auc_rf) 



# 결과 제출하기 

X_TEST = np.concatenate([X_TEST_OH, X_TEST_STD],axis = 1)

y_pred = model_rf.predict(X_TEST) 

y_score = model_rf.predict_proba(X_TEST)[:,1] 



# 문제에서 요구하는 형태로 변환 
# <제출형식> enrollee_id , target, target_prob

obj = {'enrollee_id':enrollee_id, 'target':y_pred, 'target_prob':y_score} 

result = pd.DataFrame(obj) 



# 12345.csv로 저장하기
# index = False 시 인덱스를 포함하지 않고 저장 

result.to_csv("12345.csv",index = False)  


================================================

[ Big 4th last test ]

# 연령별 운동 수행 등급과 관련한 데이터의 일부이다. 
# 주어진 데이터를 이용하여 예측 모형을 만들고 CSV 파일 생성 
# (단, 제출 전 두 개 이상의 모형의 성능을 비교하여 가장 우수한 모형을 선정할 것) 
# 랜던포레스트 외 하나 더 연습 

# (가) 제공 데이터 목록 
# (1) bodyPerfor_train.csv : 10,713명의 학습용 데이터, csv 형식의 파일  
# (2) bodyPerfor_test.csv : 2,680명의 평가용 데이터, csv 형식의 파일

# (나) 데이터 형식 및 내용

# 10,713명에 대한 학습용 데이터 (bodyPerfor_train.csv)를 이용하여
# 운동 수행 등급 예측 모형을 만든 후 이를 평가용 데이터( bodyPerfor_test.csv)에 적용하여 얻은
# 2,680명의 수행 등급 예측값을 CSV 파일로 생성하시오. 
# (* 제출한 모델의 성능은 "macro f1-score" 평가지표에 따라 채점)

# <제출형식> 
# class : 운동 수행 등급(A, B, C, D ;A가 가장 높은 등급) 

# <유의사항> 
# 성능이 우수한 예측모형을 구축하기 위해서는 적절한 데이터 전처리, Feature Engineering, 
# "분류" 알고리즘 사용, 초매개변수 최적화, 모형 앙상블 등이 수반 



import pandas as pd

train = pd.read_csv(r'D:\빅데이터\빅데이터분석기사 실기 모의고사\data\기출복원\04회\bodyPerfor_train.csv')

test = pd.read_csv(r'D:\빅데이터\빅데이터분석기사 실기 모의고사\data\기출복원\04회\bodyPerfor_test.csv')


# 확인 
print(train.head())

print(test.head())


# * train 데이터를 X_train과 y_train으로 분할 
# 제출형식이 class이므로, class를 y_train에 할당하고, X_train에서는 없앰 

X_train = train.drop(columns='class').copy()
y_train = train['class'].copy() 


# test도 같은 격식으로 X_test로 할당. 
# 기존 test 내에는 class가 없었으므로 삭제는 불필요 

X_test = test.copy()  



print(X_train.info())
print(X_test.info())
print(y_train.info())

print(X_train.describe())
print(X_test.describe())
print(y_train.describe())



# >>> 불필요한 컬럼 삭제 

# 별도 저장한 컬럼 및 불필요한 컬럼 삭제 
# id 컬럼은 개인 고유번호로 key 역할로 모델에는 불필요 하므로 삭제 

X_train = X_train.drop(columns = 'id')
X_test = X_test.drop(columns = 'id')
y_train = y_train.drop(columns = 'id') 



# >>> 결측치 처리 

# 결측치 확인 

X_train.isna().sum() 

X_test.isna().sum() 



# >> 카테고리형 컬럼 전처리 
# 문자열(object) 컬럼들의 유일값 수 확인  

print(X_train.select_dtypes('object').nunique()) 


print(X_test.select_dtypes('object').nunique()) 

# 컬럼별 카테고리 확인 결과 큰 이상 없음 



# >>> 수치형 컬럼 전처리 

# > age 컬럼 
# 구간화(binning)하여 age_gp에 할당 - dtype을 object형으로 변경 
# (구간화는 100살 기준이든 70살 기준이든)
# range(A): 0부터 A-1까지 반환 
# range(A,B): A부터 B-1까지 반환
# range(A,B,C): A부터 B-1까지 C간격으로 반환

X_train['age_gp'] = pd.cut(X_train['age'], bins=list(range(0,70,10))).astype('object')
X_test['age_gp'] = pd.cut(X_test['age'], bins=list(range(0,70,10))).astype('object') 


# 완료 후 기존 컬럼 삭제 

X_train = X_train.drop('age', axis = 1)
X_test = X_test.drop('age', axis = 1) 


# > height, weight, body_fat, diastolic, systolic, grip_force, sit_bend_forward, sit_ups, broad_jump 

# 수치형만 있는 데이터프레임 추출 
X_train_conti = X_train.select_dtypes(exclude = 'object').copy() 
X_test_conti = X_test.select_dtypes(exclude = 'object').copy() 


# 상관관계 확인, 컬럼간 강한 상관관계는 나타나지 않음 
X_train_conti.corr() 



# >>> 데이터 분할 

# X_train과 y_train을 학습용 (X_TRAIN, y_TRAIN)과 검증용 (X_VAL, y_VAL)로 분할 
# ( , , , ,) : 변수에 담기 
# 검증용 VAL은 나중에 성능평가 시에 검증용 데이터를 통한 예측을 할 때 사용함 

from sklearn.model_selection import train_test_split 


X_TRAIN, X_VAL, y_TRAIN, y_VAL = train_test_split(X_train, y_train, random_state=1234, test_size=0.3, stratify = y_train) 


# 분할 후 shpae 확인 

print(X_TRAIN.shape) 


print(X_VAL.shape)

print(y_TRAIN.shape)

print(y_VAL.shape)



# 인코딩 - 원-핫 인코딩 수행 
# 카테고리형 컬럼(object)에 대하여 원핫인코딩 수행 
# 원핫인코딩 : 범주형 데이터를 숫자로 변환하여, 딥러닝 모델에 적용 가능하게 변환함 
# (ex) 개 1000, 고양이 0100, 원숭이 0010, 돼지 0001 

from sklearn.preprocessing import OneHotEncoder 


# 인코딩 할 카테고리형(object) 컬럼만 별도 저장   
# 테스트는 스플릿이 안되어 있으므로 정의변수는 대문자, 할당 시에는 소문자 사용 

X_TRAIN_category = X_TRAIN.select_dtypes('object').copy() 
X_VAL_category = X_VAL.select_dtypes('object').copy()
X_TEST_category = X_test.select_dtypes('object').copy() 


# 원-핫 인코딩 
# 디폴트는 sparse = True이며 매트릭스 리턴 , sparse = False이면 배열 리턴 
# fit 메서드: 모델을 학습시킬 때 사용함. // 머신러닝이 데이터에 머신러닝 모델을 맞추는 것(fit) 
# transform: fit을 통해 세운 기준(모델)으로 맞춰서 변형하는 함수 

# X트레인카테고리 정의변수 활용하여 fit 기반 모델 학습  
# 스팔스_아웃풋 = 폴스로 입력함 
# handle_unknown='ignore' : fit 단계에서 배열에 없는 값이 있을 때 넣어주면 ValueError 문구 없이 정상 실행 

enc = OneHotEncoder(sparse_output=False).fit(X_TRAIN_category) 


# X트레인, X밸, X테스트에 대해서 원핫인코딩 함수 기반 트랜스폼 진행 

X_TRAIN_OH = enc.transform(X_TRAIN_category)
X_VAL_OH = enc.transform(X_VAL_category) 
X_TEST_OH = enc.transform(X_TEST_category) 



# >>> 스케일링 
# 스케일링 ;; 카테고리형이 아닌 것, 숫자 관련된 것. 수치형. 
# StandardScaler(): 표준화. 평균이 0이고 분산 1인 정규분포 만드는 것. z-score  

from sklearn.preprocessing import StandardScaler 


# 스케일링할 컬럼만 별도 저장 
# select_dtypes() 메서드의 exclude 옵션은 해당 dtype을 제외한 모든 dtypes를 추출할 때 사용 
# 테스트는 스플릿이 안되어 있으므로 정의변수는 대문자, 할당 시에는 소문자 사용 

X_TRAIN_conti = X_TRAIN.select_dtypes(exclude = 'object').copy() 
X_VAL_conti = X_VAL.select_dtypes(exclude = 'object').copy() 
X_TEST_conti = X_test.select_dtypes(exclude = 'object').copy() 


# TRAIN 데이터 기준으로 스케일링 함 
# 스탠다드스케일러에 괄호 포함 

scale = StandardScaler().fit(X_TRAIN_conti) 



# Z-점수 표준화 
# ; transform에서 평균, 분산, 표준편차 등 가지고, 거기에 때리면서 구하면서 표준화 
# transform: fit을 통해 세운 기준(모델)으로 맞춰서 변형하는 함수  

X_TRAIN_STD = scale.transform(X_TRAIN_conti)
X_VAL_STD = scale.transform(X_VAL_conti)
X_TEST_STD = scale.transform(X_TEST_conti) 



# >>> 입력 데이터셋 준비 

import numpy as np 


# 인코딩과 스케일링 된 넘파이 배열 연결  

X_TRAIN = np.concatenate([X_TRAIN_OH, X_TRAIN_STD], axis = 1) 
X_VAL = np.concatenate([X_VAL_OH, X_VAL_STD], axis = 1) 


# 'A'~'D'를 0~3으로 매핑 
# class는 운동 수행 등급으로 A~D까지 있음 (A, B, C, D ; A가 가장 높은 등급) 

y_TRAIN = y_TRAIN.map({'A':0. 'B':1, 'C':2, 'D':3}) 
y_VAL = y_VAL.map({'A':0, 'B':1, 'C':2, 'D':3})


# y를 1차원 넘파이 배열로 평탄화 
# ravel: 1차원 배열로 평평하게 펴주는 함수 

# (참고) 
# 딕셔너리는 key(), values() 
# 그 중 키만 뽑을 때 : key()
# 값만 뽑을 때 : values() 
# 키, 값 쌍으로 뽑고 싶을 때 : items() 

# 1차원 넘파이 배열로 평탄화 
y_TRAIN = y_TRAIN.values.ravel() 
y_VAL = y_VAL.values.ravel() 



# >>> 모델 학습

from sklearn.ensemble import RandomForestClassifier 


# 랜덤 포레스트 

rf = RandomForestClassifier(n_estimators=500, max_depth=3, min_samples_leaf=10, max_features='sqrt', random_state=2022) 


model_rf = rf.fit(X_TRAIN, y_TRAIN)

# fit 메서드: 모델을 학습시킬 때 사용함. // 머신러닝이 데이터에 머신러닝 모델을 맞추는 것(fit) 


# 성능평가(기준: macro f1_score) 모델 선정 // 단, 여기서는 랜덤포레스트만 사용함 
# 분류 성능 1에 가까울 수록 좋음 

from sklearn.metrics import f1_score 


# 검증용 데이터셋을 통한 예측 

pred_rf = model_rf.predict(X_VAL)  



## macro f1_score 계산 
# macro가 앞에 붙어 있으므로, 노멀 f1_score의 산식에 average='macro' 추가) 

f1_rf = f1_score(y_VAL, pred_rf, average='macro')

print(f1_rf)



# 결과 제출하기 

X_TEST = np.concatenate([X_TEST_OH, X_TEST_STD],axis = 1)

y_pred = model_rf.predict(X_TEST)



# 문제에서 요구하는 형태로 변환 
# <제출형식> class  

obj = {'class':y_pred} 

result = pd.DataFrame(obj) 

# 추가 작업 (기존 치환분 되감아주기)
result['class'] = result['class'].map({0:'A', 1:'B', 2:'C', 3:'D'}) 


# 12345.csv로 저장하기
# index = False 시 인덱스를 포함하지 않고 저장 

result.to_csv("12345.csv",index = False) 


================================================

===============================================
[Big Mo 1]

----------------------------------------------

# (문제 1) iris 데이터셋을 불러와 Sepal.Width 컬럼에 대해 
Sepal.Width의 평균값을 기준으로 3배 표준편차 이상으로 떨어진 값들의 합을 구하여라

import pandas as pd 

exam1 = pd.read_csv(r'C:\빅데이터분석기사 실기 모의고사\data\모의고사\01회\iris.csv') 

print(exam1.head())
print(exam1.info())
print(exam1.describe())


Sepal_Width = exam1['Sepal.Width'] 
# 대괄호 안에 홑따옴표  

avg = Sepal_Width.mean() 

std = Sepal_Width.std() 
# 표준편차 구할 때 std 임 


up = avg + std*3 

down = avg - std*3 

# 업이 플러스고 다운이 마이너스 

cond1 = (Sepal_Width < down) | (Sepal_Width > up)

result = Sepal_Width[cond1].sum() 
# 컨디션 적용 시 대괄호 

print(result) 

----------------------------------------------

# (문제 2) mtcars1 데이터셋을 불러와, disp 컬럼에 대해서 순위를 부여한 후, 
# 1위부터 20위까지의 값들의 표준편차를 구하고 소수점 셋째자리에서 반올림하여 나타내어라. 
# (단, 동점은 동일한 순위를 부여하되 상위 등수를 기준으로 하며 최댓값을 1위로 함) 


import pandas as pd 

exam2 = pd.read_csv(r'C:\빅데이터분석기사 실기 모의고사\data\모의고사\01회\mtcars1.csv') 

print(exam2.head())
print(exam2.info())
print(exam2.describe())


disp = exam2['disp'] 

rank = disp.rank(method='min', ascending=False)  
# 동점 동일한 순위 부여이므로 메또트는 민 적용 
# 폴스의 에프는 대문자로 기재 


# 랭크 1에서 20까지를 추리는게 필요함 
rank20 = disp[rank <= 20] 


result = round(rank20.std(), 2) 
# 리졀트에 할당 필요

print(result) 

----------------------------------------------

# (문제 3) Cars93 데이터셋을 불러와, 
# '전체 레코드 수', '결측치가 있는 컬럼의 수', '전체 결측치 수', 
# '결측치가 10개 이상인 컬럼들의 결측치가 있는 레코드만 삭제한 후의 전체 레코드 수'와 
# '두 개 이상의 컬럼이 동시에 결측인 레코드의 행번호들의 합'을 구한 후 모두 합하여라.  


import pandas as pd 

exam3 = pd.read_csv(r'C:\빅데이터분석기사 실기 모의고사\data\모의고사\01회\Cars93.csv') 

print(exam3.head())
print(exam3.info())
print(exam3.describe()) 


# 전체 레코드 수 
case1 = exam3.shape[0] 

# 대괄호 영 
# X_test.shape를 해보시면 (로우건수, 컬럼건수) 가 나옵니다. 그래서 X.shape[0]은 로우건수를 의미


# 결측치가 있는 컬럼의 수 
case2 = sum(exam3.isna().sum() != 0) 


# 전체 결측치 수 
case3 = sum(exam3.isna().sum()) 


# '결측치가 10개 이상인 컬럼들의 결측치가 있는 레코드만 삭제한 후의 전체 레코드 수'
cond10over = exam3.columns[exam3.isna().sum() > 10] 

# 그 중에 결측치가 없는 컬럼의 수

sub1 = exam3[cond10over].copy() 

case4 = len(sub1.dropna()) 
# na를 드랍하면 결측치가 없음 


# '두 개 이상의 컬럼이 동시에 결측인 레코드의 행번호들의 합'

# 조건을 먼저 셋팅. 인덱스 활용 

rownm_2over = exam3.index[exam3.isna().sum(axis=1) >= 2] 


# 행번호들을 리스트로 반환 

sub2 = list(rownm_2over) 

sub2

case5 = sum(sub2) 

result = case1 + case2 + case3 + case4 + case5 

print(result) 

===============================================

===============================================	
[Big Mo 2]

----------------------------------------------

# (문제 1) USArrests 데이터셋을 불러와, 
# UrbanPop이 60 이상인 지역 중 Murder와 Assault의 합 대비 Assault의 비율이 0.05 이상인 레코드 수를 구하여라. 

import pandas as pd

exam2_1 = pd.read_csv(r'C:\빅데이터분석기사 실기 모의고사\data\모의고사\02회\USArrests.csv') 

print(exam2_1.head())
print(exam2_1.info())
print(exam2_1.describe())


exam2_1['MA'] = exam2_1['Murder'] + exam2_1['Assault']

exam2_1['ratio'] = exam2_1['Assault'] / exam2_1['MA']


cond1 = (exam2_1['UrbanPop'] >= 60) & (exam2_1['ratio'] >= 0.05) 

result = exam2_1[cond1].shape[0]  

print(result) 

----------------------------------------------

# (문제 2) swiss 데이터셋을 불러와, Fertility 컬럼에 대해서 내림차순으로 정렬한 후 
# 정렬한 데이터를 기준으로 홀수번째 레코드들의 평균에서 짝수번째 레코드들의 평균을 뺀 값을 구하여라. 
# (단, 첫번째 행에 있는 데이터를 1번으로 하고, 결과는 소수점 넷째 자리에서 반올림하여 표현) 

import pandas as pd

exam2 = pd.read_csv(r'C:\빅데이터분석기사 실기 모의고사\data\모의고사\02회\swiss.csv') 

print(exam2.head())
print(exam2.info())  
# ; RangeIndex: 47
print(exam2.describe())

exam2.shape[0]


sort = exam2['Fertility'].sort_values(ascending = False, ignore_index = True) 


import numpy as np 

idx = np.arange(1,48) 


odd = (idx % 2 == 1)

even = (idx % 2 == 0)  

diff = sort[odd].mean() - sort[even].mean() 

result = round(diff, 3) 
# 반올림은 라운드 함수

print(result) 

----------------------------------------------

# (문제 3) CO2 데이터셋을 불러와, 
# Type컬럼이 Mississippi이면서 conc컬럼에서 백의 자리 또는 일의 자리가 5인 경우 레코드들의 수를 구하여라. 

import pandas as pd

exam3 = pd.read_csv(r'C:\빅데이터분석기사 실기 모의고사\data\모의고사\02회\CO2.csv') 

print(exam3.head())
print(exam3.info())
print(exam3.describe()) 


exam3['Type'].value_counts()


exam3['Type'] = exam3['Type'].str.replace('/' , '') 

case1 = ( exam3['Type'] == 'Mississippi') 


hundred = (exam3['conc']//100 == 5) 

one = exam3['conc'].astype('string').str.endswith('5') 

case2 = hundred | one 


result = exam3[case1 & case2].shape[0] 
print(result) 

===============================================

===============================================	
[Big Mo 3]

----------------------------------------------

# (문제 1) Rabbit 데이터셋을 불러와 'Dose 컬럼'의 제 3사분위수와 제2분위수를 구하고 
# 두 값의 차이의 절댓값을 구한 후 소수점을 버린 값을 출력하여라. 

import pandas as pd 

exam1 = pd.read_csv(r'C:\빅데이터분석기사 실기 모의고사\data\모의고사\03회\Rabbit.csv')

print(exam1.head())
print(exam1.info())
print(exam1.describe())

q3 = exam1['Dose'].quantile(0.75)

q2 = exam1['Dose'].median()  

diff = abs(q3 - q2) 

result = diff.astype('int64') 

print(result)

----------------------------------------------

# (문제 2) Boston 데이터셋을 불러와 'medv 컬럼'에 대해서 동일한 폭으로 binning(구간화) 한 후 
# 가장 많은 빈도를 가지는 구간을 산출하고 해당 구간 내 dis컬럼의 중앙값을 구하여라. 
# (폭은 10을 기준으로 하고 소수점은 둘째 자리까지 나타내시오)  

import pandas as pd

exam2 = pd.read_csv(r'C:\빅데이터분석기사 실기 모의고사\data\모의고사\03회\Boston.csv') 

print(exam2.head())
print(exam2.info())
print(exam2.describe())


exam2['medv'].value_counts()
# 이렇게 하면 50까지 나오는 걸 볼 수 있음 

medv_cut = pd.cut(exam2['medv'], bins = [0, 10, 20, 30, 40, 50]) 


mode = medv_cut.value_counts().idxmax() 

cond = (medv_cut == mode) 


median = exam2['dis'][cond].median()  

result = round(median, 2) 

print(result)  

----------------------------------------------

# (문제 3) Melanoma 데이터셋을 불러와 1번째~122번째 레코드와 123번째 이후 레코드로 데이터셋을 분리하고 
# 각 데이터셋별로 'thickness 칼럼'을 z-score 정규화로 변환한 후 -1과 1 사이 값들의 중앙값을 각각 산출한 후, '합계'를 구하여라. 
# (단, z-score 정규화 변환 계산에 사용되는 평균과 표준편차는 분리된 것과 관계 없이 
# 1번째~122번째 레코드로 이루어진 데이터셋을 기준으로 하고, 
# 출력 시 소수점 넷째자리'까지' 반올림하여 나타낼 것)


import pandas as pd 

exam3 = pd.read_csv(r'C:\빅데이터분석기사 실기 모의고사\data\모의고사\03회\Melanoma.csv')


print(exam3.head())
print(exam3.info())
print(exam3.describe())


df1 = exam3.loc[:123] 

df2 = exam3.loc[123:] 


avg = df1['thickness'].mean() 

std = df1['thickness'].std() 


z1 = (df1['thickness'] - avg)/std 

z2 = (df2['thickness'] - avg)/std 


sub_z1 = (z1 >= -1) & (z1 <= 1)

sub_z2 = (z2 >= -1) & (z2 <= 1)


med1 = sub_z1.median()

med2 = sub_z2.median()


result = round(med1 + med2, 4) 
print(result) 

===============================================	

===============================================	
[Big P 2]

----------------------------------------------

# (문제 1) mtcars2 데이터셋을 불러와 mpg 컬럼의 상위 10번째 값으로 상위 10개 값을 변환한 후 
# drat가 4 이상인 값에 대해 mpg의 평균을 구하여라.

import pandas as pd

exam1 = pd.read_csv(r'C:\빅데이터분석기사 실기 모의고사\data\기출복원\02회\mtcars2.csv')

print(exam1.head())
print(exam1.info())
print(exam1.describe())


mpg = exam1['mpg'].copy() 

drat = exam1['drat'].copy() 


# 상위 10번째 값 
tenth = mpg.sort_values(ascending=False, ignore_index=True)[9] 


# 상위 10개 인덱스 
idx = mpg.sort_values(ascending=False)[:10].index


# 상위 10개 값 변경 
mpg[idx] = tenth 


# drat가 4이상일 때 mpg의 평균 
avg_mpg = mpg[drat>=4].mean()  

result = round(avg_mpg, 3)

print(result)

----------------------------------------------

# (문제 2) 앞의 데이터셋(mtcars2)을 새로 불러와 첫 번째 행부터 순서대로 80%까지의 데이터를 훈련데이터로 추출한 후 
# disp 컬럼의 결측값을 disp 컬럼의 중앙값으로 대체하고, 대체 전 후의 disp 변수의 표준편차 값의 차이를 구하여라.
# (단, 차이는 빼는 순서와 관계없이 절대값을 취하여 표시하라)


import pandas as pd 

import numpy as np 

exam2 = pd.read_csv(r'C:\빅데이터분석기사 실기 모의고사\data\기출복원\02회\mtcars2.csv')

idx80 = np.floor(exam2.shape[0] * 0.8).astype('int') 


# 80%가 되는 행 인덱스 번호 찾기 
idx80 = np.floor(exam2.shape[0] * 0.8).astype('int') 
# (round는 반올림, ceil은 올림, floor는 내림)

# 훈련 데이터 추출 
train = exam2[:idx80] 


# disp 변수의 결측값을 중앙값으로 대체, 대체 전후 표준편차 

# disp 컬럼 할당 
disp_before = train['disp'].copy()  # 대체하지 않을 것 

disp_after = train['disp'].copy()   # 대체할 것 


# 대체 전의 중앙값과 표준편차 

med_before = disp_before.median()

std_before = disp_before.std() 


# 결측치 중앙값 대체 
disp_after = disp_after.fillna(med_before) 

# 대체 후의 표준편차 
std_after = disp_after.std() 


# 차이 (표준편차 간)
diff = abs(std_before - std_after) 

result = round(diff, 3)

print(result)

----------------------------------------------

# (문제 3) gehan 데이터셋을 불러와 time 컬럼에서 이상값의 합을 구하여라. 
# (단, 이상값은 평균에서 1.5 표준편차 이상으로 벗어난 값으로 정의함) 

import pandas as pd 

exam3 = pd.read_csv(r'C:\빅데이터분석기사 실기 모의고사\data\기출복원\02회\gehan.csv') 

print(exam3.head())
print(exam3.info())


# time 컬럼의 평균과 표준편차 (쭉) 

time = exam3['time']

avg_time = time.mean()

std_time = time.std() 



# 평균에서 1.5 표준편차 구간 

low = avg_time - std_time*1.5 

up = avg_time + std_time*1.5 


# 이상치 
outlier = time[(time < low) | (time > up)] 


result = sum(outlier) 

print(result)

===============================================	
